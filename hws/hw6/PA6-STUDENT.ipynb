{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PA6 - Random Projection Distortions and KNN Classifier\n",
    "\n",
    "## Random Projections and their Distortion\n",
    "\n",
    "#### Euclidean distance of two vectors\n",
    "The distance of two vectors, x and y, each of which are of dimension d is given by the following formula:\n",
    "\n",
    "$$E_d(A,B) = \\sqrt{\\sum_i^d (a_i - b_i)^2}$$\n",
    "\n",
    "This formula can be easily computed in Python np.linalg.norm function described below.\n",
    "\n",
    "#### Useful Linear Algebra functions in Python:\n",
    "1. array.shape() - returns a tuple containing the shape of \"array\"\n",
    "2. array.T - returns the transpose of \"array\" (i.e if \"array\" is a P x Q matrix, the transpose would be a Q x P matrix where A[i,j] = A[j,i] for all $1 \\leq i \\leq P$ and for all $1 \\leq j \\leq Q$)\n",
    "3. np.linalg.norm(array) - computes the l2 norm of \"array\" by default. Documentation can be found <a href = \"https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html\" title=\"here\"> here </a>\n",
    "\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "In the first part of this assignment, you are given a data set in the form of a $n \\times d$ matrix, where $n$ represents the number of data points, and $d$ represents the dimensions of each data point.\n",
    "\n",
    "You are tasked with reducing the dimensionality of the data set using the Johnson-Lindenstrauss Lemma and to measure the average distortion accross different projection dimensions.\n",
    "\n",
    "### Dimension Reduction\n",
    "\n",
    "Suppose we are given n points where each point, $x_i$ where $1 \\leq i \\leq n$ is a massive vector in the Euclidean space $\\mathbb{R}^d$ (i.e. each point is a massive vector with d elements). We are interested in all-pairwise Euclidean distances. Being able to work with vectors in lower dimensions would mean less computation, and therefore overall faster execution. \n",
    "\n",
    "The question becomes: how do we reduce \"d\" while still maintaining a good approximation between all pairs of Euclidean distances? The answer was given by Johnson and Lidenstrauss in the form of the following lemme we saw in class.\n",
    "\n",
    "#### Johnson Lidenstrauss lemma\n",
    "\n",
    "Let $\\epsilon \\in (0, \\frac{1}{2})$, Then for **any** set of points $S = \\{x_1, ..., x_n\\}$ in $\\mathbb{R}^d$, there exists a mapping, $A: \\mathbb{R}^d \\to \\mathbb{R}^k$ where $k = O(\\frac{log n}{\\epsilon^2})$ s.t.\n",
    "\n",
    "$$\\forall x_1, x_2 \\in S: (1-\\epsilon)   \\| x_1 - x_2 \\|  \\leq \\|Ax_1 - Ax_2\\| \\leq (1+\\epsilon)   \\|x_1 - x_2 \\| $$\n",
    "\n",
    "Notice in the lemma above that $k$ does not actually depend on $d$, but only on $n$ and $\\epsilon$\n",
    "\n",
    "#### The transformation mapping, A\n",
    "\n",
    "In class you saw that the entries of matrix $A$ are drawn from a Normal distribution $\\mathcal{N}(0,1)$, and then this matrix is multiplied by $\\frac{1}{\\sqrt k}$. An alternative construction that is suprisingly simple due to Achlioptas is the following: the mapping, A, is a $k \\times d$ matrix, where each entry in the matrix is randomly assigned as $-\\frac{1}{\\sqrt k}$ or $+\\frac{1}{\\sqrt k}$ with equal probability. The JL lemma still holds. \n",
    "\n",
    "### Distortion\n",
    "\n",
    "In many practical scenarios, we want to try dimensions $k$ less than the value required by the JL lemma. We are no longer guaranteed that $\\frac{\\|Ax_1 - Ax_2\\| }{\\sqrt{k}}$ will be within a $(1\\pm \\epsilon)$ factor of the true distance between $x_1,x_2$.  We refer to the differences in distance between transformed points and their respective original points as **distortion**. Specifically, we have the following definition of the **distortion** caused by the embedding, A:\n",
    "\n",
    "$$distortion_A(x,y) = | \\frac{\\|Ax - Ay\\|}{\\|x-y\\|} - 1|$$\n",
    "\n",
    "$$distortion(A) = \\max_{x,y} \\ distortion_A(x,y)$$\n",
    "\n",
    "Here, $\\mathcal{D}$ is our dataset.   The first formula describes the distortion between a given pair of points, cause by the projection A.  The second formula defines the distortion of the mapping, A over the whole set of points in $\\mathcal{D}$, and is just the maximum distortion of any two points from  $\\mathcal{D}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick question (2 points each, Double Click to Edit): \n",
    "\n",
    "#### 1. When is $distortion(A) = 0$?\n",
    "\n",
    "#### 2. Comment on the overal trend of the distortion as a function of target_dimension. Reference the graph you plotted below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import cell\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "import pandas as pd\n",
    "\n",
    "def load(data_path):\n",
    "    data = np.load(data_path)\n",
    "    return data\n",
    "\n",
    "data = load(\"./gastro_data.npy\") #assumes file is in same directory as ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mapping(original_data, target_dimension):\n",
    "    '''\n",
    "    Input:\n",
    "        Original data: N x D np array\n",
    "        params: dictionary contatining the two parameters \"dtype\" and \"target_dimension\"\n",
    "        \n",
    "    Output:\n",
    "        D x K np array, where K = \"target_dimension\" from params and the elements follow\n",
    "        the mapping, A, described in the problem\n",
    "    '''\n",
    "    pass\n",
    "\n",
    "def project(original_data, mapping):\n",
    "    '''\n",
    "    \n",
    "    Apply the mapping to the N x D original_data, returning the N x K projection where K << D\n",
    "    Input:\n",
    "        Original data: R x D np array\n",
    "        mapping: D x K np array from \"create_mapping\" function\n",
    "        \n",
    "    Output:\n",
    "        N x K np array where each row is the random projection of the original D dimension vector. \n",
    "    '''\n",
    "    pass\n",
    "\n",
    "\n",
    "def all_distortions(original_data, new_data, random_sample = None):\n",
    "    '''\n",
    "    A helper function for \"calculate_distortion\"\n",
    "    \n",
    "    Calculate all pairwise distortions of an embedding and return as a List\n",
    "    \n",
    "    Input:\n",
    "        original_data: N x D matrix representing the original data\n",
    "        new_data: N x K matrix, where K << D, represending the data with random projection applied\n",
    "        \n",
    "    Output:\n",
    "        List - containing all distortions values for all data points in any order. \n",
    "    '''\n",
    "    pass\n",
    "\n",
    "def calculate_distortion(original_data, new_data, random_sample = None):\n",
    "    '''\n",
    "    calculate the distortion of an embedding.\n",
    "\n",
    "    Input:\n",
    "        original_data: N x D matrix representing the original data\n",
    "        new_data: N x K matrix, where K << D, represending the data with random projection applied\n",
    "        \n",
    "    Output:\n",
    "        Maximum distortion for the random projection that created \"new data\". \n",
    "        \n",
    "    '''\n",
    "    pass\n",
    "\n",
    "def plot_distortion(original_data, list_of_target_dimensions=[5,10,20,50]):\n",
    "    '''\n",
    "    Plot the average distortion for the given list_of_target_dimensions\n",
    "    \n",
    "    Input:\n",
    "        list_of_target_dimensions: list where each element represents a target dimensions to project the data onto\n",
    "        original data: N x D matrix \n",
    "        \n",
    "    Output:\n",
    "        No return\n",
    "        display a graph using matplotlib where the x-axis is the target dimensions,\n",
    "        and Y is the average distortion accross 5 trials for that specific target dimension.\n",
    "    '''\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effects of Random Projection on a K-Nearest Neighbor (KNN) Classifier\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "In the previous problem we looked at how using random projections can cause some distortion between the pair wise distances of any two points in the data set. In this problem we will be looking at a classification algorithm that uses pair wise distances to decide what class a point belongs to; the classification algorithm we will be analyzing is the KNN classifier. \n",
    "\n",
    "You will be able to observe, first hand, the effects that random projections has on both the performance (accuracy, recall, precision, f1 score) of the classifier, as well as the speed of the classifier. \n",
    "\n",
    "#### Data Set\n",
    "You are given 4 files:\n",
    "1. GENE_train_data.npy - n x d numpy array, which represents the training data for the classifier\n",
    "2. GENE_train_data.npy - n x 1 numpy array, which represents the true labels of the trianing data\n",
    "3. GENE_test_data.npy - m x d numpy array, which represents the testing data for the classifier\n",
    "4. GENE_test_labels.npy - m x 1 numpy array, which represents the true labels of the testing data\n",
    "\n",
    "n = 640, m = 160, and d = 16,383\n",
    "\n",
    "\n",
    "## K-Nearest Neighbor Classifier\n",
    "\n",
    "For a KNN, you are given a training and testing data set. As the names suggest, your training will be used to \"train\" your classifier, and testing will be used to assess the classifier.\n",
    "\n",
    "For each data point in your testing set, you will iterate through the training set and return a class decision for the testing point. The class decision will be based on the majority classes of the K \"nearest neighbors\" to the testing point. \"Nearest\" is defined as the shorted Euclidean distance. \n",
    "\n",
    "For this problem you have the option of implementing your own KNN classifier, or you can use the built in KNN classifier in SKLearn. You can read more about SKLearn's KNN <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\" title=\"here\"> here </a>\n",
    "\n",
    "For both your own implementation and the build in KNN, please set the num_neighbors = 5\n",
    "\n",
    "## Metrics\n",
    "\n",
    "Here are four common metrics used to assess a classification model:\n",
    "1. Precision - <a href=\"https://en.wikipedia.org/wiki/Precision_and_recall\" title='pr'> more information </a>\n",
    "2. Recall - <a href=\"https://en.wikipedia.org/wiki/Precision_and_recall\" title='pr'> more information </a>\n",
    "3. F1 Score - <a href=\"https://en.wikipedia.org/wiki/F-score\" title='pr'> more information </a>\n",
    "4. Accuracy - formula is \"guessed correct / total number of samples\"\n",
    "\n",
    "## Data Description\n",
    "\n",
    "The data set we are working with is collected by analyzing the gene expression of patients diagnosed with different types of tumors. The types of tumors analyzed in this model are given by their shorthands: BRCA, KIRC, COAD, LUAD, PRAD. (Feel free to look up these short hands to learn more about what types of tumors this data set deals with). Over 16,000 gene expressions were analyzed for every patient.\n",
    "\n",
    "For this problem, we use a KNN + training data to classify newly diagnosed patients into one of these categories. In order to make application of KNN easier,I have replaced the class name labels with class ID labels using the following scheme. {'BRCA': 0, 'COAD': 1, 'KIRC': 2, 'LUAD': 3, 'PRAD': 4}\n",
    "\n",
    "When answering the written questions at the end of the homework, you should take into consideration the data your classifier operates on. \n",
    "\n",
    "## Code Base\n",
    "\n",
    "You are given the following:\n",
    "1. load - A function that loads all data and labels into memory\n",
    "2. metrics - A function that computes metrics given a prediction array and truth array\n",
    "    \n",
    "You are asked to complete the following:\n",
    "1. originalDataKNN(train, train_labels, test, test_labels)\n",
    "2. randomProjectionKNN(train, train_labels, test, test_labels)\n",
    "3. Three written questions to be answered at the end\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import time\n",
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "#warnings.filterwarnings(\"ignore\")  <- supresses python warnings. feel free to leave commented or uncomment\n",
    "labels = [0,1,2,3,4] #global var used for metrics function\n",
    "\n",
    "def load(train_data_path, train_labels_path, test_data_path, test_labels_path):\n",
    "    '''\n",
    "    Input:\n",
    "        train_data_path - path to given .npy file\n",
    "        train_labels_path - path to given .npy file\n",
    "        test_data_path - path to given .npy file\n",
    "        test_labels_path - path to given .npy file\n",
    "        \n",
    "    Output:\n",
    "        train - np array\n",
    "        train_labels - np array\n",
    "        test - np array\n",
    "        test_labels - np array\n",
    "    '''\n",
    "    train = np.load(train_data_path)\n",
    "    train_labels = np.load(train_labels_path)\n",
    "    test = np.load(test_data_path)\n",
    "    test_labels = np.load(test_labels_path)\n",
    "    \n",
    "    return train, train_labels, test, test_labels\n",
    "\n",
    "def metrics(truth, prediction):\n",
    "    '''\n",
    "    Input:\n",
    "        truth: Numpy array w/ dimensions M x 1 Where each element is the true class of the respective point in \"test\"\n",
    "        prediction: Numpy array w/ dimensions M x 1 where each element is the predicted class of the respective point in \"test\"\n",
    "    Output:\n",
    "        accuracy, precision, recall, and f1 score of prediction vs truth\n",
    "    '''\n",
    "    accuracy = sklearn.metrics.accuracy_score(truth, prediction) \n",
    "    prec = sklearn.metrics.precision_score(truth, prediction, labels=labels, average='micro')\n",
    "    recall = sklearn.metrics.recall_score(truth, prediction, labels=labels, average='micro')\n",
    "    f1 = 2*(prec *recall) / (prec + recall)\n",
    "    return accuracy, prec, recall, f1\n",
    "\n",
    "def create_mapping(original_data, target_dimension):\n",
    "    '''\n",
    "    copy your code from the first problem here for convenience \n",
    "    \n",
    "    Input:\n",
    "        Original data: N x D np array\n",
    "        params: dictionary contatining the two parameters \"dtype\" and \"target_dimension\"\n",
    "        \n",
    "    Output:\n",
    "        D x K np array, where K = \"target_dimension\" from params and the elements follow the mapping, A, described in the problem\n",
    "    '''\n",
    "    pass\n",
    "\n",
    "def project(original_data, mapping):\n",
    "    '''\n",
    "    copy your code from the first problem here for convenience \n",
    "    \n",
    "    Input:\n",
    "        Original data: R x D np array\n",
    "        mapping: D x K np array from \"create_mapping\" function\n",
    "        \n",
    "    Output:\n",
    "        N x K np array where each row is the random projection of the original D dimension vector. \n",
    "    '''\n",
    "    pass\n",
    "\n",
    "#assumes all files are within the same directory as ipynb. If not, change the directory setup or adjust str paths below\n",
    "train, train_labels, test, test_labels = load(train_data_path = \"./GENE_train_data.npy\", train_labels_path = \"./GENE_train_labels.npy\", test_data_path = \"./GENE_test_data.npy\", test_labels_path = \"./GENE_test_labels.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell for KNN implementation\n",
    "'''\n",
    "Implement your KNN in this cell\n",
    "'''\n",
    "num_neighbors = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def originalDataKNN(train, train_labels, test, test_labels):\n",
    "    '''\n",
    "    Input:\n",
    "        train: N x D matrix representing original training data\n",
    "        train_labels: N - length np vector, representing labels for training data\n",
    "        test: M x D matrix, representing original testing data\n",
    "        test_labels: M - length np vector representing labels for testing data\n",
    "    \n",
    "    Output:\n",
    "        None\n",
    "        \n",
    "    In this cell, we ask you to run a KNN classifier on the original data set. \n",
    "    \n",
    "    num_neighbors = 5\n",
    "\n",
    "    For full credit make sure the following is computed and printed:\n",
    "        1. The run time of the KNN\n",
    "            a. we define run time as: before KNN classifier is created to after the metrics are computed\n",
    "        2. The metrics of the classifier on the test data set. \n",
    "            a. metrics should be clearly printed and labled as to which number corresponds with which metric\n",
    "    '''\n",
    "    num_neighbors = 5\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomProjectionKNN(train, train_labels, test, test_labels):\n",
    "    '''\n",
    "    Input:\n",
    "        train: N x D matrix representing original training data\n",
    "        train_labels: N - length np vector, representing labels for training data\n",
    "        test: M x D matrix, representing original testing data\n",
    "        test_labels: M - length np vector representing labels for testing data\n",
    "    \n",
    "    Output:\n",
    "        None\n",
    "        \n",
    "    In this cell, we ask you to run a KNN classifier on the reduced data set via random projections. \n",
    "\n",
    "    For each dimensions, repeat the random projection + classification 5 times.\n",
    "    \n",
    "    num_neighbors = 5\n",
    "\n",
    "    Please assess the classifier accross the following reduced dimensions: [25,50,100,200]\n",
    "\n",
    "    For full credit make sure the following is computed and printed:\n",
    "        1. For each reduced dimension listed above, clearly print and label the average run time and all average metrics\n",
    "        2. 2 graphs need to be displayed using matplotlib.plt\n",
    "            a. Graph 1: x-axis: Reduced dimension, y-axis: average run time accross 5 trials\n",
    "            b. Graph 2: x-axis: reduced dimension, y-axis: average accuracy accross 5 trials\n",
    "            c. Graphs should contain an approriate title, and axis labels\n",
    "            \n",
    "    Note: when measuring time for the classifier on the projected data, you should also include the time it took to\n",
    "    project the data to the lower dimensionality as part of the overall run time.\n",
    "    '''\n",
    "\n",
    "    reduced_dimensions = [25,50,100,200]\n",
    "    num_neighbors = 5\n",
    "    num_trials = 5 \n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis (Double Click to Edit)\n",
    "\n",
    "1. What effect did the random projection dimensionality reduction have on the run time of the classifier?\n",
    "2. What effect did the random projection dimensionality reduction have on the performance of the classifier?\n",
    "3. Do you think the extra work needed to project the data to a lower dimensionality was worth it? In your answer, discuss the speed and performance of the classifier, as well as the space needed to store the data. In addition, provide evidence to back your claim. (Note: full score given as long as answer has sufficient evidence behind it). \n",
    "\n",
    "For each answer, refer to your graphs as evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
